<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta charset="utf-8">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@400;500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  :root {
    --container-max: 960px;
    --space-xxs: 4px;
    --space-xs: 8px;
    --space-s: 12px;
    --space-m: 16px;
    --space-l: 24px;
    --space-xl: 32px;
  }
  a {
    color: #1772d0;
    text-decoration:none;
    font-size: 16px;
  }
  a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
  }
  body,td,th {
    font-family: 'Fira Sans', Arial, sans-serif;
    font-size: 16px;
    font-weight: 400;
    line-height: 1.5;
    color: #222
  }
  heading {
    font-family: 'Fira Sans', Arial, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Fira Sans', Arial, sans-serif;
    font-size: 14px;
    font-weight: 800
  }
  strongred {
    font-family: 'Fira Sans', Arial, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Fira Sans', Arial, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Fira Sans', Arial, sans-serif;
    font-size: 38px;
    font-weight: 400
  }

  /* added by yc */

  padding {
    padding: 50px 0px;
  }

  /* Global rhythm and spacing */
  p { margin: 0 0 var(--space-m); }
  /* Ê†áÈ¢òÂ≠óÂè∑Ë∞ÉÂ§ß‰∏Ä‰∫õÔºåËÆ©È£éÊ†ºÊõ¥Êé•Ëøë qinren È°µÈù¢ */
  h2 { margin: var(--space-xl) 0 var(--space-m); font-weight: 700; line-height: 1.3; font-size: 1.6rem; }
  heading, sectionheading, pageheading { display: block; }
  heading { margin: var(--space-s) 0 var(--space-xs); line-height: 1.35; font-size: 1.6rem; }
  sectionheading { margin: var(--space-l) 0 var(--space-s); line-height: 1.4; font-size: 1.8rem; }
  pageheading { margin: var(--space-l) 0 var(--space-s); line-height: 1.2; font-size: 2.8rem; }
  ul { margin: 0 0 var(--space-m) 20px; }
  ul.other-interests li { margin-bottom: 6px; }

  /* Tables as layout containers */
  table { border-collapse: separate; border-spacing: 0; }
  td { vertical-align: top; }

  /* Paper blocks */
  .paper p { margin: 0 0 var(--space-s); }

  /* News box styling */
  .news-box {
    background: #fafbfd;
    border: 1px solid #e8ecf3;
    padding: var(--space-s) var(--space-m) var(--space-m);
    border-radius: 10px;
    box-shadow: 0 1px 2px rgba(0,0,0,0.03);
    max-height: 250px;
    overflow-y: auto;
  }
  .news-box ul { margin: 0; padding-left: 18px; }
  .news-box li { margin: 6px 0; }
  .news-box sectionheading { margin-top: 0; }

  /* Images */
  img { border-radius: 12px; }

  /* Section spacing helpers */
  p[align="center"] { margin-bottom: var(--space-m); }
  table[width="100%"][cellpadding="10"] { margin-top: var(--space-l); }
  .news-box { margin: var(--space-m) 0 var(--space-l); }

  /* Publications list styling */
  #pubs tr + tr td { padding-top: var(--space-l); border-top: 1px solid #e8ecf3; }
  #pubs td:first-child { width: 32% !important; }
  #pubs td:nth-child(2) { width: 68% !important; }

  /* Tighter spacing between title and authors in publications */
  #pubs heading { margin: 0 0 0px; font-size: 1.3rem; line-height: 1.3; }
  #pubs heading + br { display: none; }

  </style>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üí•</text></svg>" type="image/svg+xml">
  <!-- Removed viewport meta to match desktop-style scaling behavior on mobile (similar to index_yifan.html) -->
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->
  <link rel="apple-touch-icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>‚ö°Ô∏èÔ∏è</text></svg>">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Boya Liao</title>
  <meta name="Boya Liao's Homepage" http-equiv="Content-Type" content="Boya Liao's Homepage">
  <script src="js/scramble.js"></script>
</head>

<body>
<!-- <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<table width="960" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Boya Liao</pageheading><br>    
  </p>

  <tr>
    <td width="38%" valign="top"><a href="img/profile.png"><img src="img/profile.png" width="100%" style="border-radius:15px"></a>
        <p align=center>
          <a href="https://www.linkedin.com/in/boya-liao-176b02390/">LinkedIn</a> |
          <a href="https://scholar.google.com/citations?user=xdWZc2YAAAAJ">Google Scholar</a> |
          <a href="https://github.com/boyaliao7">Github</a> |
          <a href="https://x.com/Boyalike7">X</a> |
          <!-- <a href="mailto:buronliao@gmail.com">Email</a> -->
      </p>
    </td>

    <td width="62%" valign="top" align="justify">
        <div class="profile-text">
            <p>Welcome to my homepage üëã. I'm Boya Liao.
              I‚Äôm currently a research intern at Westlake University, advised by Prof. <a href="https://huanwang.tech/">Huan Wang</a>. </p>
            
            <p>Previously, I completed my M.E. in Computer Technology from Zhejiang University, where I was advised by Prof. <a href="https://person.zju.edu.cn/en/msong">Mingli Song</a> in the <a href="https://www.vipazoo.cn/">VIPA</a> lab. Before that, I earned my B.E. in Computer Science and Technology from Chang‚Äôan University.</p>
        
            <p>üî• I am seeking for PhD position in 26Fall. Please feel free to reach out to me via email if you believe I am a good fit for your research team. I welcome the opportunity for further discussion!</p>
              
            <b>Email</b>: bursonliao _at_ gmail.com
            <font id="email" style="display:inline;">
              <noscript><i>Please enable Javascript to view</i></noscript>
            </font>
          </div>

    </td>

  </tr>
</table>

<h2>Research Interests</h2>
<section class="content-section"> 
            <p>My primary research goal is to develop <b>scalable, reliable, and efficient methods</b> for machine learning and generative AI, with a particular focus on sparsity, adaptive representation learning, and principled uncertainty estimation in foundation models‚Äîincluding LLMs, VLMs, and diffusion models. In addition, I am also highly interested in:
              <ul class="other-interests">
                  <li>üìö Memorization in large models</li>
                  <li>üîÑ Self-consuming/self-improving loops</li>
                  <li>ü§ñ Agent learning with Foundation models</li>
              </ul>

            </section>
<!-- <h2>News</h2>
<div class="service-list news-list">
      <ul>
      <li><strong>[10/2025]</strong> Join <a href="https://bytedancebandai.notion.site/intro"> ByteDance Bandai</a> as a Research Intern, targeting Deep Research!‚ö°Ô∏èÔ∏è‚ö°Ô∏è  </li>  
      <li><strong>[07/2025]</strong> The wait is over! <a href="https://arxiv.org/abs/2503.01776">CSR</a> is now available in  <a href="https://huggingface.co/blog/train-sparse-encoder">Sentence-Transformer v5.0!</a> ü§óü§ó</li>  
      <li><strong>[05/2025]</strong> Our paper <a href="https://arxiv.org/abs/2503.01776">CSR</a> has been accepted at ICML 2025 <font color="red">(Oral)</font>! üéâüéâ </li>
      <li><strong>[03/2025]</strong> Our recent work <a href="https://arxiv.org/abs/2503.01776">Contrastive Sparse Representation</a> has <a href="https://x.com/yifeiwang77/status/1897023662328611062">generated considerable interest</a> as a promising alternative approach for efficient embedding retrieval, 
        and we have been invited to publish the model on Hugging Face and Sentence-Transformer! Code available at <a href="https://github.com/neilwen987/CSR_Adaptive_Rep">Link</a>. ü§óü§ó</li>
      <li><strong>[02/2025]</strong> Our paper <a href="https://arxiv.org/abs/2503.18483">LanCE</a> has been accepted at CVPR 2025! üéâüéâ </li>
      <li><strong>[07/2024]</strong> Our paper <a href="https://arxiv.org/abs/2407.18589">HICE-Score</a> accepted by ACM MM 2024! üéâüéâ</li>
      </ul>
</div>
<br>     -->
<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;Publications</sectionheading></td></tr>
</table> -->

<h2>Publications</h2>
<table id="pubs" width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  
  <tr>
    <td width="30%" valign="top" align="center">
        <a href="#">
        <img src="img/csr.png" alt="sym" width="400" height="200" style="border-radius:15px">
        </a>
    </td>

    <td width="60%" valign="top">
        <p>
          <papertitle>
            <a href="https://arxiv.org/abs/2503.01776">Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</a>
          </papertitle>
            <div class="paper-info">
              <strong>Tiansheng Wen<sup>*</sup></strong>, Yifei Wang<sup>*</sup>, Zequn Zeng, Zhong Peng, Yudi Su Xinyang Liu,Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You
            </div>
            <div class="paper-info">
              <span style="padding: 4px 8px; background: #e0e7ff; color: #3730a3; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #c7d2fe;">ICML 2025</span>
              <span style="padding: 4px 8px; background: #fef3c7; color: #92400e; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #fde68a;">Oral Presentation</span>  
            </div>
            <!-- <b><font color="red">Oral Presentation</font></b>  -->
        </p>
        <div class="paper" id="csr">
        <a href="javascript:toggleblock('CSR_abs')", class="big-link">abstract</a> |
        <a href="https://arxiv.org/abs/2503.01776", class="big-link">paper</a> |
        <a href="https://huggingface.co/blog/train-sparse-encoder", class="big-link">Sentence-Transformer Blog</a> |
        <a href="https://github.com/neilwen987/CSR_Adaptive_Rep", class="big-link">code</a> 
        <a href="https://github.com/neilwen987/CSR_Adaptive_Rep" target="_blank">
          <img
            src="https://img.shields.io/github/stars/neilwen987/CSR_Adaptive_Rep?style=social"
            alt="GitHub Repo stars"
            style="margin-left: 4px; vertical-align: middle;"
          >
        </a>

        <p align="justify">
            <i id="CSR_abs">
              Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval,
                search, and generative modeling.
                Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it
                requires full model retraining and suffers from noticeable performance degradations at short lengths.
                In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation
                with minimal overhead and higher fidelity.
                We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a
                high-dimensional but selectively activated feature space.
                By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic
                quality while allowing flexible, cost-effective inference at different sparsity levels.
                Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently
                outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting
                training time to a fraction of that required by MRL.
                Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world
                applications where efficiency and fidelity are both paramount.
            </i>
        </p>

        </div>
    </td>
  </tr>

  <tr>
    <td width="30%" valign="top" align="center">
        <a href="#">
        <img src="img/seqtopk.svg" alt="sym" width="300" height="250" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <!-- <td width="70%" valign="top">
        <p>
          <papertitle>
            <a href="https://arxiv.org/abs/2511.06494">Route Experts by Sequence, not by Token</a>
          </papertitle>
          <div class="paper-info">
            <strong>Tiansheng Wen<sup>*</sup></strong>, Yifei Wang<sup>*</sup>, Aosong Feng, Long Ma, Xinyang Liu, Yifan Wang, Lixuan Guo, Bo Chen, Stefanie Jegelka, Chenyu You
          </div>
            <div class="paper-info">
              <span style="padding: 4px 8px; background: #f3f4f6; color: #4b5563; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #d1d5db;">Preprint 2025</span>  
            </div>
        </p>
        <div class="paper" id="seqtopk">
        <a href="javascript:toggleblock('seqtopk_abs')", class="big-link">abstract</a> |
        <a href="https://arxiv.org/abs/2511.06494", class="big-link">paper</a> |
        <a href="https://github.com/Y-Research-SBU/SeqTopK", class="big-link">code</a>  -->

        <p align="justify">
            <i id="seqtopk_abs">
              Mixture-of-Experts (MoE) architectures scale large language models (LLMs) by activating
              only a subset of experts per token, but the standard <em>TopK</em> routing assigns the same 
              fixed number of experts to all tokens, ignoring their varying complexity. Prior adaptive 
              routing methods introduce additional modules and hyperparameters, often requiring costly 
              retraining from scratch. We propose <strong>Sequence-level TopK (SeqTopK)</strong>, 
              a minimal modification that shifts the expert budget from the token level to the sequence 
              level. By selecting the top T &times; K experts across all T tokens, SeqTopK enables 
              end-to-end learned dynamic allocation -- assigning more experts to difficult tokens and 
              fewer to easy ones -- while preserving the same overall budget. SeqTopK requires only a 
              few lines of code, adds less than 1% overhead, and remains fully compatible with pretrained 
              MoE models. Experiments across math, coding, law, and writing show consistent improvements 
              over TopK and prior parameter-free adaptive methods, with gains that become substantially 
              larger under higher sparsity (up to 16.9%). These results highlight SeqTopK as a simple, 
              efficient, and scalable routing strategy, particularly well-suited for the extreme sparsity 
              regimes of next-generation LLMs.
            </i>
        </p>

        </div>
    </td>
  </tr>

  <tr>
    <td width="30%" valign="top" align="center">
        <a href="#">
        <img src="img/csrv2.jpg" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="70%" valign="top">
        <p>
          <papertitle>
            <a href="https://neilwen987.github.io/">CSRV2: Unlocking Ultra-Sparse Embeddings</a>
          </papertitle>
          <div class="paper-info">
            Lixuan Guo<sup>*</sup>, Yifei Wang<sup>*</sup>, <strong>Tiansheng Wen<sup>*</sup></strong>, Yifan Wang, Aosong Feng, Bo Chen, Stefanie Jegelka, Chenyu You
          </div>
            <div class="paper-info">
              <span style="padding: 4px 8px; background: #f3f4f6; color: #4b5563; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #d1d5db;">Preprint 2025</span>  
            </div> 
        </p>
        <div class="paper" id="csrv2">
        <a href="javascript:toggleblock('csrv2_abs')", class="big-link">abstract</a> |
        <a href="https://neilwen987.github.io/", class="big-link">paper</a> |
        <a href="https://github.com/Y-Research-SBU/CSRv2/tree/main", class="big-link">code</a> 

        <p align="justify">
            <i id="csrv2_abs">
              In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability.
              Yet widely used dense embeddings are often extremely high-dimensional (e.g., 4096), incurring substantial costs in storage, memory, and inference latency.
              To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but <i>k</i>-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL).
              Despite its promise, CSR suffers severe degradation in the ultra-sparse regime (e.g., <i>k</i> &leq; 4), where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized.
              In this paper, we introduce <strong>CSRv2</strong>, a principled training approach designed to make ultra-sparse embeddings viable.
              CSRv2 stabilizes sparsity learning through progressive <i>k</i>-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning.
              CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at <i>k</i>=2, bringing ultra-sparse embeddings on par with CSR at <i>k</i>=8 and MRL at 32 dimensions, <em>all with only two active features</em>.
              While maintaining comparable performance, CSRv2 delivers a {7&times; speedup over MRL}, and yields up to <strong>300&times; improvements in compute and memory efficiency</strong> relative to dense embeddings.
              Extensive experiments across text (MTEB, multiple state-of-the-art LLM embeddings (Qwen and e5-Mistral-7B)) and vision (ImageNet-1k) demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance.
              By making extreme sparsity viable, CSRv2 broadens the design space for large-scale, real-time, and edge-deployable AI systems where both embedding quality and efficiency are critical.
            </i>
        </p>

        </div>
    </td>
  </tr>
  
  
  <tr>
    <td width="30%" valign="top" align="center">
        <a href="#">
        <img src="img/lance.png" alt="sym" width="400" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="70%" valign="top">
        <p>
          <papertitle>
            <a href="https://arxiv.org/abs/2503.18483">Explaining Domain Shifts in Language: Concept erasing for Interpretable Image Classification</a>
          </papertitle>
          <div class="paper-info">
            Zequn Zeng<sup>*</sup>, Yudi Su<sup>*</sup>, Jianqiao Sun,<strong> Tiansheng Wen</strong>, Hao Zhang, Zhengjue Wang, Bo Chen, Hongwei Liu, Jiawei Ma
            <div class="paper-info">
              <span style="padding: 4px 8px; background: #e0e7ff; color: #3730a3; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #c7d2fe;">CVPR 2025</span> 
            </div>
          </div>
        </p>

        <div class="paper" id="eegformer">
        <a href="javascript:toggleblock('lance_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2503.18483", class="big-link">paper</a> |
        <a href="https://github.com/joeyz0z/LanCE", class="big-link">code</a> 
        <a href="https://github.com/joeyz0z/LanCE" target="_blank">
          <img
            src="https://img.shields.io/github/stars/joeyz0z/LanCE?style=social"
            alt="LanCE GitHub Repo stars"
            style="margin-left: 4px; vertical-align: middle;"
          >
        </a>

        <p align="justify">
            <i id="lance_abs">
                Concept-based models can map black-box representations to human-understandable concepts,
                which makes the decision-making process more transparent and then allows users to understand the reason behind predictions.
                However, domain-specific concepts often impact the final predictions, which subsequently undermine the model generalization capabilities, and prevent the model from being used in high-stake applications.
                In this paper, we propose a novel Language-guided Concept-Erasing (LanCE) framework.
                In particular, we empirically demonstrate that pre-trained vision-language models (VLMs) can approximate
                distinct visual domain shifts via domain descriptors while prompting large Language Models (LLMs) can easily
                simulate a wide range of descriptors of unseen visual domains. Then, we introduce a novel plug-in domain
                descriptor orthogonality (DDO) regularizer to mitigate the impact of these domain-specific concepts on the
                final predictions. Notably, the DDO regularizer is agnostic to the design of concept-based models and we
                integrate it into several prevailing models. Through evaluation of domain generalization on four standard
                benchmarks and three newly introduced benchmarks, we demonstrate that DDO can significantly improve the
                out-of-distribution (OOD) generalization over the previous state-of-the-art concept-based model.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/cfa.JPG" alt="sym" width="400" height="100" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="70%" valign="top">
        <p>
          <papertitle>
            <a href="https://arxiv.org/abs/2407.21740">Contrastive Factor Analysis</a>
          </papertitle>
          <div class="paper-info">
            Zhibin Duan<sup>*</sup>, <strong>Tiansheng Wen<sup>*</sup></strong>, Yifei Wang, Chen Zhu, Bo Chen, Mingyuan Zhou
          </div>
            <div class="paper-info">
              <span style="padding: 4px 8px; background: #f3f4f6; color: #4b5563; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #d1d5db;">Preprint 2024</span>  
            </div>
            <!-- <em>arXiv</em> / TPAMI (Under Review) -->
        </p>

        <div class="paper" id="eegformer">
        <a href="javascript:toggleblock('cfa_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2401.10278", class="big-link">paper</a> 

        <p align="justify">
            <i id="cfa_abs">
              Factor analysis, often regarded as a Bayesian variant of matrix factorization,
                offers superior capabilities in capturing uncertainty, modeling complex dependencies,
                and ensuring robustness. As the deep learning era arrives, factor analysis is receiving
                less and less attention due to their limited expressive ability. On the contrary,
                contrastive learning has emerged as a potent technique with demonstrated efficacy
                in unsupervised representational learning. While the two methods are different paradigms,
                recent theoretical analysis has revealed the mathematical equivalence between contrastive
                learning and matrix factorization, providing a potential possibility for factor analysis
                combined with contrastive learning. Motivated by the interconnectedness of contrastive
                learning, matrix factorization, and factor analysis, this paper introduces a novel
                Contrastive Factor Analysis framework, aiming to leverage factor analysis's advantageous
                properties within the realm of contrastive learning. To further leverage the interpretability
                properties of non-negative factor analysis, which can learn disentangled representations,
                contrastive factor analysis is extended to a non-negative version. Finally, extensive
                experimental validation showcases the efficacy of the proposed contrastive (non-negative)
                factor analysis methodology across multiple key properties, including expressiveness,
                robustness, interpretability, and accurate uncertainty estimation.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/hice.png" alt="sym" width="350" height="300" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="70%" valign="top">
        <p>
          <papertitle>
            <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681358">HICEScore: A Hierarchical Metric for Image Captioning Evaluation</a>
          </papertitle>
          <div class="paper-info">
            Zequn Zeng, Jianqiao Sun, Hao Zhang, <strong>Tiansheng Wen</strong>, Yudi Su, Yan Xie, Zhengjue Wang, Bo Chen
          </div>
            <div class="paper-info">
              <span style="padding: 4px 8px; background: #e0e7ff; color: #3730a3; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #c7d2fe;">ACM MM 2024</span> 
            </div>
        </p>

        <div class="paper" id="Veatic">
<!--        <a href="https://veatic.github.io/">webpage</a> |-->
        <a href="javascript:toggleblock('hice_abs')", class="big-link">abstract</a> |
        <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681358", class="big-link">paper</a> |
        <a href="https://github.com/joeyz0z/HICE", class="big-link">code</a> 
        <a href="https://github.com/joeyz0z/HICE" target="_blank">
          <img
            src="https://img.shields.io/github/stars/joeyz0z/HICE?style=social"
            alt="HICE GitHub Repo stars"
            style="margin-left: 4px; vertical-align: middle;"
          >
        </a>

        <p align="justify">
            <i id="hice_abs">
Image captioning evaluation metrics can be divided into two categories, reference-based metrics and reference-free metrics. However, reference-based approaches may struggle to evaluate descriptive captions with abundant visual details produced by advanced multimodal large language models, due to their heavy reliance on limited human-annotated references. In contrast, previous reference-free metrics have been proven effective via CLIP cross-modality similarity. Nonetheless, CLIP-based metrics, constrained by their solution of global image-text compatibility, often have a deficiency in detecting local textual hallucinations and are insensitive to small visual objects. Besides, their single-scale designs are unable to provide an interpretable evaluation process such as pinpointing the position of caption mistakes and identifying visual regions that have not been described. To move forward, we propose a novel reference-free metric for image captioning evaluation, dubbed Hierarchical Image Captioning Evaluation Score (HICE-S). By detecting local visual regions and textual phrases, HICE-S builds an interpretable hierarchical scoring mechanism, breaking through the barriers of the single-scale structure of existing reference-free metrics. Comprehensive experiments indicate that our proposed metric achieves the SOTA performance on several benchmarks, outperforming existing reference-free metrics like CLIP-S and PAC-S, and reference-based metrics like METEOR and CIDEr. Moreover, several case studies reveal that the assessment process of HICE-S on detailed captions closely resembles interpretable human judgments.
                Our code is available at https://github.com/joeyz0z/HICE.
            </i>
        </p>

        </div>
    </td>
  </tr>

    <tr>
    <td width="35%" valign="top" align="center">
        <a href="#">
        <img src="img/gammavae.png" alt="sym" width="400" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="70%" valign="top">
        <p>
          <papertitle>
            <a href="https://arxiv.org/abs/2408.03388">A Non-negative VAE: the Generalized Gamma Belief Network</a>
          </papertitle>
          <div class="paper-info">
            Zhibin Duan, <strong>Tiansheng Wen</strong>, Muyao Wang, Bo Chen, Mingyuan Zhou
          </div>
            <div class="paper-info">
              <span style="padding: 4px 8px; background: #f3f4f6; color: #4b5563; border-radius: 12px; font-size: 0.85rem; font-weight: 600; border: 1px solid #d1d5db;">Preprint 2024</span>  
            </div>
            <!-- <em>arXiv</em> / TPAMI (Major Revision) -->
        </p>

        <div class="paper" id="Veatic">
<!--        <a href="https://veatic.github.io/">webpage</a> |-->
        <a href="javascript:toggleblock('gammavae_abs')", class="big-link">abstract</a> |
        <a href="https://arxiv.org/abs/2408.03388", class="big-link">paper</a> 
<!--        <a href="https://github.com/joeyz0z/HICE">code</a> |-->

        <p align="justify">
            <i id="gammavae_abs">
The gamma belief network (GBN), often regarded as a deep topic model, has demonstrated its potential for uncovering multi-layer interpretable latent representations in text data. Its notable capability to acquire interpretable latent factors is partially attributed to sparse and non-negative gamma-distributed latent variables. However, the existing GBN and its variations are constrained by the linear generative model, thereby limiting their expressiveness and applicability. To address this limitation, we introduce the generalized gamma belief network (Generalized GBN) in this paper, which extends the original linear generative model to a more expressive non-linear generative model. Since the parameters of the Generalized GBN no longer possess an analytic conditional posterior, we further propose an upward-downward Weibull inference network to approximate the posterior distribution of the latent variables. The parameters of both the generative model and the inference network are jointly trained within the variational inference framework. Finally, we conduct comprehensive experiments on both expressivity and disentangled representation learning tasks to evaluate the performance of the Generalized GBN against state-of-the-art Gaussian variational autoencoders serving as baselines.
            </i>
        </p>

        </div>
    </td>
  </tr>




<!-- <table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <h2>Professional Activity</h2>
        <ul>
          <li> Conference Reviewer: NeurIPS, ICLR, CVPR, AAAI</li>
          <li> Journal Reviewer: TPAMI, JMLR, PR, TMI, TNNLS.</li>

        </ul>
    </td>
  </tr>
</table> -->

<!--<table width="100%" align="center" border="0" cellpadding="10">-->
<!--  <tr>-->
<!--    <td>-->
<!--      <sectionheading>&nbsp;&nbsp;Awards</sectionheading>-->
<!--        <ul>-->
<!--          <li> [06/2024] I received the honor of being the <b>Outstanding Graduate</b> in ShanghaiTech. </li>-->
<!--          <li> [12/2023] I received the honor of being 2022-2023 <b>Merit Student</b> in ShanghaiTech. </li>-->
<!--          <li> [07/2023] I received the <b>Undergraduate International Exchange Special Scholarship</b> in ShanghaiTech. </li>-->
<!--          <li> [12/2022] I received the honor of being 2021-2022 <b>Merit Student</b> in ShanghaiTech. </li>-->
<!--        </ul>-->
<!--    </td>-->
<!--  </tr>-->
<!--</table>-->

<!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=n&d=5Mbudn0FSMUmDjHq2NxUjvoq_vySIg_giud-A4yWPHk"></script> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr><td><br><p align="center"><font size="2">
    Template from this <a href="https://soonera.github.io/qinren/">awesome website</a>.
    </font></p></td></tr>
    
</table>
  </td> </tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('CSR_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('seqtopk_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('csrv2_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('lance_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('cfa_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('hice_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('gammavae_abs');
</script>
</body>

</html>
